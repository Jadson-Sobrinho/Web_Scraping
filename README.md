# DocumentaÃ§Ã£o do Projeto

## Ferramentas e Bibliotecas Utilizadas

### 1. ManipulaÃ§Ã£o de Arquivos e DiretÃ³rios
- **`os`**: Criar diretÃ³rios e manipular caminhos
- **`pathlib.Path`**: Gerenciar caminhos de forma organizada
- **`glob`**: Buscar arquivos com padrÃµes especÃ­ficos
- **`zipfile`**: Compactar/descompactar arquivos ZIP

### 2. Web Scraping e HTTP
- **`requests`**: RequisiÃ§Ãµes HTTP
- **`BeautifulSoup (bs4)`**: Parsing de HTML

### 3. ManipulaÃ§Ã£o de Dados
- **`pandas`**: Processamento de dados estruturados
- **`pdfplumber`**: ExtraÃ§Ã£o de dados de PDFs

### 4. Banco de Dados
- **`sqlite3`**: Banco de dados SQLite
- **`pandas.to_sql()`**: InserÃ§Ã£o de dados no banco

### 5. Framework Web (API)
- **`Flask`**: Servidor web e APIs
- **`flask.Blueprint`**: ModularizaÃ§Ã£o de rotas
- **`flask.Response`**: Respostas JSON
- **`flask.request`**: Captura de parÃ¢metros

### 6. Outros
- **`sys`**: ManipulaÃ§Ã£o de sys.path
- **`json`**: ManipulaÃ§Ã£o de JSON

---

## ðŸ”„ Fluxo Principal

### 1. Web-Scraping
- Acessar site governamental
- Localizar e baixar Anexos I e II
- Salvar em diretÃ³rio especÃ­fico
- Compactar em .zip

### 2. TransformaÃ§Ã£o de Dados
- Extrair dados do Anexo I
- Salvar em CSV
- Substituir abreviaÃ§Ãµes (OD â†’ OdontolÃ³gico, AMB â†’ Ambulatorial)
- Compactar em .zip

### 3. Banco de Dados
- Baixar arquivos dos Ãºltimos 2 anos
- Descompactar e mesclar trimestres
- Baixar dados de operadoras ativas
- Mesclar datasets pelo Registro ANS
- Carregar no SQLite

### 4. API
- Endpoints:
  - Top 10 operadoras com maiores despesas (Ãºltimo trimestre)
  - Top 10 operadoras com maiores despesas (Ãºltimo ano)
  - Busca por razÃ£o social/CNPJ/data/Registro ANS

---

## Estrutura de Pastas

```plaintext
WEB_SCRAPING/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/              # Rotas da API
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py               # AplicaÃ§Ã£o Flask
â”‚   â”‚   â”‚   â”œâ”€â”€ database/            # ConfiguraÃ§Ãµes do banco
â”‚   â”‚   â”‚   â”œâ”€â”€ datasets/            # Arquivos de entrada/saÃ­da
â”‚   â”œâ”€â”€ download/                    # Scripts de download
â”‚   â”œâ”€â”€ ETL/                         # Scripts de transformaÃ§Ã£o
â”œâ”€â”€ config/                          # ConfiguraÃ§Ãµes
â”œâ”€â”€ frontend/                        # Frontend (futura implementaÃ§Ã£o)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ Requirements.txt
â”œâ”€â”€ TesteTecnico.postman_collection.json
```
## DocumentaÃ§Ã£o dos cÃ³digos
  1. download/rol_procedimentos.py:

    requests.get(url): Acessa a pÃ¡gina do governo e obtÃ©m seu conteÃºdo HTML.

    BeautifulSoup(response.text, 'html.parser'): Analisa o HTML da pÃ¡gina.

    soup.find_all('a', href=True): Busca todos os links da pÃ¡gina.

    pdf_link.startswith('http'): Garante que o link seja absoluto.

    requests.get(pdf_link): Faz o download do PDF.

    zipfile.ZipFile(zip, "w"): Compacta todos os arquivos baixados em um ZIP.
    

  2. ETL/rol_procedimentos_transform.py:
      ```
      extract_data_into_dt(): LÃª todas as pÃ¡ginas do PDF e extrai as tabelas.

      normalize_df(): Junta os dados extraÃ­dos, substitui valores e os salva em um CSV.

      zip_file(): Compacta o arquivo CSV gerado.
      


  3. download/demonstracoes_contabeis.py: 
      ```
      requests.get(year_url): ObtÃ©m o conteÃºdo da pÃ¡gina HTML do diretÃ³rio de arquivos.

      BeautifulSoup(response.text, 'html.parser'): Analisa o HTML da pÃ¡gina.

      for link in soup.find_all("a", href=True): Percorre todos os links disponÃ­veis na pÃ¡gina.

      file_name.endswith(".zip"): Filtra apenas os arquivos ZIP.

      requests.get(zip_url, stream=True): Faz o download do arquivo ZIP.

      zipfile.ZipFile(zip_path, 'r').extractall(output_dir): Extrai os arquivos ZIP para o diretÃ³rio de saÃ­da.
      ```

  4. download/operadoras_ativas.py:
      ```
      requests.get(url, timeout=10): Faz uma requisiÃ§Ã£o HTTP para acessar a pÃ¡gina da ANS.

      BeautifulSoup(response.text, 'html.parser'): Analisa o cÃ³digo HTML da pÃ¡gina.

      soup.find_all("a", href=True): Busca todos os links na pÃ¡gina.

      file_name.endswith(".csv"): Filtra apenas os arquivos com extensÃ£o CSV.

      requests.get(file_url, stream=True): Faz o download do arquivo CSV.

      os.makedirs(output_dir, exist_ok=True): Garante que o diretÃ³rio de saÃ­da exista antes de salvar os arquivos.
      ```

  5. ETL/relatorio_operadoras_csv_load.py:
      ```
      merge_files(): Encontra e combina arquivos CSV de operadoras ativas.

      load_datas(): Carrega os dados dos arquivos CSV principais.

      merge_datasets(): Faz a junÃ§Ã£o dos dados normalizando valores numÃ©ricos.

      save_query_result(): Exporta o dataframe final para um CSV.
      ```

  6. ETL/relatorio_normalizado_database_load:
      ```
      sys.path.append(...): Adiciona o caminho do banco para importaÃ§Ã£o.

      get_db_connection(): Estabelece conexÃ£o com o banco.

      pd.read_csv(..., chunksize=2000): LÃª o CSV em blocos de 2000 linhas.

      to_sql(): Insere os dados na tabela empresa.

      conn.close(): Fecha a conexÃ£o apÃ³s a importaÃ§Ã£o.
      ```

  7. api/src/app.py:
      ```
      register_blueprint() para modularizar as rotas.

      ```

## TODO's
  Criar validaÃ§Ã£o das entradas que vem pela requisiÃ§Ã£o;
  Trocar o recebimento de requisiÃ§Ãµes da URL pelo JSON;
  NÃ£o mesclar os dados das operadoras de plano de saÃºde ativa e demonstraÃ§Ãµes contabeis, colocar cada arquivo como uma tabela para otimizar a consulta de dados;
  Utilizar Spark para grande quantidade de dados;
  Refatorar as query's;
  Pesquisar por alguma ORM;
  Integrar ao front-end com framework vue.js
